{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2654c8fc-63c1-4f60-aea4-ae1029d1ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file EURUSD_M1_202010010001_202210312359.csv\n",
      "Converting file GBPUSD_M1_202010010001_202210312359.csv\n",
      "Converting file USDTRY_M1_202010010001_202210312359.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yunus\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:241: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import constants as c\n",
    "\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "RAW_DATA_FOLDER = c.RAW_DATA_FOLDER\n",
    "RAW_FREQUENCY = c.RAW_FREQUENCY\n",
    "PATCH_SIZE= c.PATCH_SIZE\n",
    "FORECAST_HORIZON = c.FORECAST_HORIZON\n",
    "LOOKBACK_COEFFICIENT = c.LOOKBACK_COEFFICIENT\n",
    "THRESHOLD_STATIC_SENSITIVITY = c.THRESHOLD_STATIC_SENSITIVITY\n",
    "\n",
    "NR_OF_BINS = c.NR_OF_BINS\n",
    "\n",
    "\n",
    "PATCH_SAMPLE_RATE = c.PATCH_SAMPLE_RATE\n",
    "POOL_SIZE = c.POOL_SIZE\n",
    "\n",
    "DATETIME_FEATURES = c.DATETIME_FEATURES\n",
    "CONVERTED_DATA_FOLDER = c.CONVERTED_DATA_FOLDER \n",
    "\n",
    "\n",
    "iNrOfLookbackPatches = int((FORECAST_HORIZON*LOOKBACK_COEFFICIENT)/PATCH_SIZE)\n",
    "iNrOfForecastPatches = int(FORECAST_HORIZON/PATCH_SIZE)\n",
    "\n",
    "aLookbackTimeSteps = list(range(-(LOOKBACK_COEFFICIENT*FORECAST_HORIZON) , 0))\n",
    "aForecastTimeSteps = list(range(1, FORECAST_HORIZON + 1))\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(CONVERTED_DATA_FOLDER) == True:\n",
    "    shutil.rmtree(CONVERTED_DATA_FOLDER)\n",
    "\n",
    "os.makedirs(CONVERTED_DATA_FOLDER)\n",
    "\n",
    "aFileNames = os.listdir(RAW_DATA_FOLDER)\n",
    "for sFileName in aFileNames:\n",
    "    \n",
    "    print(f'Converting file {sFileName}')\n",
    "\n",
    "    dfRaw = pd.read_csv(\n",
    "        f'{RAW_DATA_FOLDER}\\\\{sFileName}', \n",
    "        delimiter='\\t',\n",
    "        usecols=['<DATE>', '<TIME>','<HIGH>', '<VOL>']\n",
    "    )\n",
    "\n",
    "\n",
    "    dfRaw.loc[:, 'TIME_STAMP'] = dfRaw.loc[:, '<DATE>'] + ' ' +dfRaw.loc[:, '<TIME>']\n",
    "    dfRaw.drop(['<DATE>', '<TIME>'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    dfRaw.rename(columns = {'<HIGH>':'TICKER',\n",
    "                         '<VOL>':'OBSERVED'\n",
    "                        }, inplace = True)\n",
    "\n",
    "    dfRaw.loc[:, 'TIME_STAMP'] = pd.to_datetime(dfRaw.loc[:,'TIME_STAMP'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate static and dynamic digits of tickers\n",
    "    def truncate(n, decimals=0):\n",
    "        decimals = int(decimals)\n",
    "        multiplier = 10 ** decimals\n",
    "        return ((n * multiplier).astype(int)) / multiplier\n",
    "\n",
    "    # find static digit with finest granual format\n",
    "    iMaxNrOfDecimals = 5 #  ??? should calculate automatically\n",
    "    iDecimals = iMaxNrOfDecimals\n",
    "\n",
    "    while True:\n",
    "        converted = truncate(dfRaw.loc[:,'TICKER'],iDecimals)\n",
    "\n",
    "        aDiff=  converted.diff().dropna()\n",
    "\n",
    "        aDiff[aDiff!=0] = 1\n",
    "        aDiff= aDiff.astype(int)\n",
    "\n",
    "        fSensitivity = (aDiff.sum()/aDiff.shape[0])\n",
    "\n",
    "\n",
    "        if fSensitivity <=THRESHOLD_STATIC_SENSITIVITY:\n",
    "            break\n",
    "        else:\n",
    "            iDecimals = iDecimals - 1\n",
    "\n",
    "\n",
    "    dfRaw.loc[:,'STATIC_TICKER'] = truncate(dfRaw.loc[:,'TICKER'],iDecimals)\n",
    "    dfRaw.loc[:, 'DYNAMIC_TICKER'] = (dfRaw.loc[:, 'TICKER'] - dfRaw.loc[:, 'STATIC_TICKER'])\n",
    "    \n",
    "    \n",
    "\n",
    "    ix = pd.date_range(\n",
    "        start = dfRaw.loc[:, 'TIME_STAMP'].min()+ pd.Timedelta(f'{-1}{RAW_FREQUENCY}'),\n",
    "        end = dfRaw.loc[:, 'TIME_STAMP'].max()+ pd.Timedelta(f'{-1}{RAW_FREQUENCY}'),\n",
    "        freq=f'{PATCH_SIZE}{RAW_FREQUENCY}'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dfLookbackTickers = pd.DataFrame(index = ix, columns  = aLookbackTimeSteps, dtype = 'float64')\n",
    "    dfObserveds = pd.DataFrame(index = ix, columns  = aLookbackTimeSteps, dtype = 'float64')\n",
    "    for i in aLookbackTimeSteps:\n",
    "        ixSearch = ix + pd.Timedelta(f'{i}{RAW_FREQUENCY}')\n",
    "\n",
    "        ### ???  search by considering weekend & vacations\n",
    "        dfFound = dfRaw.query('TIME_STAMP in @ixSearch')\n",
    "\n",
    "        msk = np.in1d(ixSearch.to_numpy(),dfFound.loc[:, 'TIME_STAMP'].to_numpy())\n",
    "\n",
    "        dfLookbackTickers.loc[msk, i] = dfFound.loc[:, 'TICKER'].to_numpy(dtype = 'float64')\n",
    "        dfObserveds.loc[msk, i] = dfFound.loc[:, 'OBSERVED'].to_numpy(dtype = 'float64')\n",
    "\n",
    "\n",
    "\n",
    "        dfForecastTickers = pd.DataFrame(index = ix, columns  = aForecastTimeSteps, dtype = 'float64')\n",
    "        \n",
    "        \n",
    "    for i in aForecastTimeSteps:\n",
    "        ixSearch = ix + pd.Timedelta(f'{i}{RAW_FREQUENCY}')\n",
    "\n",
    "        ### ???  search by considering weekend & vacations\n",
    "        dfFound = dfRaw.query('TIME_STAMP in @ixSearch')\n",
    "\n",
    "        msk = np.in1d(ixSearch.to_numpy(),dfFound.loc[:, 'TIME_STAMP'].to_numpy())\n",
    "\n",
    "        dfForecastTickers.loc[msk, i] = dfFound.loc[:, 'TICKER'].to_numpy(dtype = 'float64')\n",
    "\n",
    "\n",
    "\n",
    "    # handle missing data\n",
    "    dfLookbackTickers.dropna(inplace = True)\n",
    "    dfForecastTickers.dropna(inplace = True)\n",
    "    dfObserveds.dropna(inplace = True)\n",
    "    ix = np.intersect1d(dfLookbackTickers.index, dfForecastTickers.index)\n",
    "    ix = np.intersect1d(ix, dfObserveds.index)\n",
    "\n",
    "    dfObserveds = dfObserveds.loc[ix]\n",
    "    dfLookbackTickers = dfLookbackTickers.loc[ix]\n",
    "    dfForecastTickers = dfForecastTickers.loc[ix]\n",
    "    dfWholeTickers = dfLookbackTickers.merge(right= dfForecastTickers, left_index = True, right_index = True, how=  'inner')\n",
    "\n",
    "    # identify static digits\n",
    "    dfStaticDigits = truncate(dfLookbackTickers, iDecimals)\n",
    "\n",
    "    # identify number of transitions\n",
    "    dfTransitions = ((dfStaticDigits.max(axis = 1) - dfStaticDigits.min(axis = 1))/(10 ** -iDecimals)).astype(int).to_frame()\n",
    "    dfTransitions.rename(columns = {0:'NR_OF_TRANSITIONS'}, inplace = True)\n",
    "    \n",
    "    # identify dynamic digits\n",
    "    dfLookbackDynamicDigits = dfLookbackTickers.sub(dfStaticDigits.min(axis = 1),  axis = 0)\n",
    "    dfForecastDynamicDigits = dfForecastTickers.sub(dfStaticDigits.min(axis = 1),  axis = 0)\n",
    "    dfWholeDynamicDigits = dfLookbackDynamicDigits.merge(right = dfForecastDynamicDigits, left_index = True, right_index = True, how = 'inner')\n",
    "\n",
    "\n",
    "    # static covariates\n",
    "    dfStaticCovariates = dfStaticDigits.min(axis = 1).to_frame()\n",
    "    dfStaticCovariates.rename(columns = {0:'STATIC_DIGIT'}, inplace = True)\n",
    "    dfStaticCovariates = dfStaticCovariates.merge(right = dfTransitions, left_index = True, right_index = True, how = 'inner')\n",
    "    arr = dfStaticCovariates.to_numpy(dtype = 'float64')\n",
    "    \n",
    "    aStaticCovariates = arr.copy()\n",
    "    \n",
    "    \n",
    "    # observeds\n",
    "    arr = dfObserveds.to_numpy(dtype = 'float64')\n",
    "    arr = np.reshape(arr , (arr.shape[0], iNrOfLookbackPatches ,-1))\n",
    "\n",
    "    arr = np.quantile(arr , [0.1, 0.5, 0.9], axis = 2)\n",
    "    arr =np.transpose(arr ,(1, 2, 0))\n",
    "    \n",
    "    aObserveds = arr.copy()\n",
    "    \n",
    "    \n",
    "    # knowns\n",
    "    ix = dfWholeTickers.index\n",
    "    arr=  np.zeros(shape = (ix.shape[0], iNrOfLookbackPatches + iNrOfForecastPatches, len(DATETIME_FEATURES) ))\n",
    "\n",
    "    for i in range(-iNrOfLookbackPatches, iNrOfForecastPatches):\n",
    "        ixSearch = ix + pd.Timedelta(f'{i*PATCH_SIZE}{RAW_FREQUENCY}')\n",
    "\n",
    "        j = 0\n",
    "        for sDatePart in DATETIME_FEATURES:\n",
    "            exec(f'arr[:, {i} ,{j}] = ixSearch.{sDatePart}')\n",
    "\n",
    "            j = j + 1\n",
    "                        \n",
    "    aKnowns = arr.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # distribution representations\n",
    "    dfLookbackNormalization = (dfWholeTickers.sub(dfLookbackTickers.min(axis = 1), axis = 0))\n",
    "    dfLookbackNormalization = dfLookbackNormalization.div(dfLookbackTickers.max(axis = 1)- dfLookbackTickers.min(axis = 1), axis = 0) \n",
    "    \n",
    "    dfLookbackNormalization[dfLookbackNormalization>1] = 1 #in case forecast horizon has increasing trend\n",
    "    dfLookbackNormalization[dfLookbackNormalization<0] = 0 #in case forecast horizon has decreasing trend\n",
    "    \n",
    "    oBinsDiscretizer = KBinsDiscretizer(n_bins=NR_OF_BINS, encode = 'ordinal', strategy='uniform')\n",
    "    arr = dfLookbackNormalization.to_numpy(dtype = 'float64')\n",
    "    arr = np.transpose(arr)\n",
    "    arr = oBinsDiscretizer.fit_transform(arr)\n",
    "    arr = np.transpose(arr).astype(int)\n",
    "\n",
    "    arr = np.reshape(arr , (arr.shape[0], iNrOfLookbackPatches + iNrOfForecastPatches ,-1))\n",
    "\n",
    "    arr = np.apply_along_axis(lambda x: np.bincount(x, minlength=NR_OF_BINS), 2, arr) \n",
    "\n",
    "    arr =(arr/PATCH_SIZE) # probabilities\n",
    "    \n",
    "    arr = np.around(arr, decimals = 2) #rounded to 2\n",
    "    \n",
    "    aDistribution = arr.copy()\n",
    "    \n",
    "    \n",
    "    # dynamic digit representation\n",
    "    arr = dfWholeDynamicDigits.to_numpy(dtype = 'float64')\n",
    "        # also should be normalized.\n",
    "    arr = np.reshape(arr , (arr.shape[0], iNrOfLookbackPatches + iNrOfForecastPatches ,-1))\n",
    "\n",
    "    arr = np.quantile(arr , [0.1, 0.5, 0.9], axis = 2)\n",
    "    arr =np.transpose(arr ,(1, 2, 0))\n",
    "    \n",
    "    aDynamicDigits = arr.copy()\n",
    "    \n",
    "    \n",
    "    # trend & seasonality\n",
    "    arr = dfWholeTickers.to_numpy()\n",
    "    arr = np.reshape(arr , (arr.shape[0], iNrOfLookbackPatches + iNrOfForecastPatches ,-1))\n",
    "    arr2 = np.subtract(arr ,  np.expand_dims(np.min(arr , axis = 2), 2))\n",
    "    arr2 = np.divide(arr2 ,  np.expand_dims(np.max(arr , axis = 2) - np.min(arr , axis = 2), 2))\n",
    "    arr2 = np.reshape(arr2, (arr2.shape[0], - 1))\n",
    "    dfSelfPatchNormalization = pd.DataFrame(data = arr2, index = dfWholeTickers.index, columns = dfWholeTickers.columns)\n",
    "\n",
    "    arr = dfSelfPatchNormalization.to_numpy()\n",
    "    arr = np.reshape(arr , (arr.shape[0], iNrOfLookbackPatches + iNrOfForecastPatches,-1))\n",
    "\n",
    "    avg_pool_sampling = tf.keras.layers.AveragePooling1D (pool_size=int(PATCH_SAMPLE_RATE * PATCH_SIZE),padding='same')\n",
    "    arr  = np.transpose(arr,  (0,2,1))\n",
    "    arr = np.around(arr, decimals = 2) #rounded to 2\n",
    "    aSample = avg_pool_sampling(arr)\n",
    "\n",
    "    avg_pool_trend = tf.keras.layers.AveragePooling1D (pool_size=POOL_SIZE, strides = 1 ,padding='same')\n",
    "    arr = avg_pool_trend(aSample)\n",
    "    arr = np.around(arr, decimals = 2) #rounded to 2\n",
    "    aTrend = arr.copy()\n",
    "    \n",
    "    aSample  = np.transpose(aSample,  (0,2,1))\n",
    "    aTrend  = np.transpose(aTrend,  (0,2,1))\n",
    "    \n",
    "    arr = aSample - aTrend\n",
    "    arr = np.around(arr, decimals = 2) #rounded to 2\n",
    "    aSeasonality = arr.copy()\n",
    "    \n",
    "\n",
    " \n",
    "    # quantiles\n",
    "    arr = dfForecastDynamicDigits.to_numpy(dtype = 'float64')\n",
    "    arr = np.reshape(arr , (arr.shape[0], iNrOfForecastPatches, -1))\n",
    "\n",
    "    arr = np.quantile(arr , [0.1, 0.5, 0.9], axis = 2)\n",
    "    arr =np.transpose(arr ,(1, 2, 0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    aQuantiles = arr.copy()\n",
    "    \n",
    "    \n",
    "    # deltas\n",
    "    arr = dfWholeTickers.to_numpy(dtype = 'float64')\n",
    "    arr = np.reshape(arr , (arr.shape[0], iNrOfLookbackPatches + iNrOfForecastPatches, -1))\n",
    "    arr = np.quantile(arr , [0.1, 0.5, 0.9], axis = 2)\n",
    "    arr =np.transpose(arr ,(1, 2, 0))\n",
    "    arrDiff = np.roll(arr,  shift = -1, axis=1)-arr #last position of axis 1 should be disregarded.\n",
    "    arr= arrDiff/arr\n",
    "    arr[:, -1,:] = 0 #last position of axis 1 should be disregarded. We set 0 for them to have standard shape.\n",
    "    aDeltas = arr.copy()\n",
    "    \n",
    "    \n",
    "    # timestamps\n",
    "    aTimestamps = dfForecastTickers.index.to_numpy()\n",
    "    \n",
    "    \n",
    "    sChannelId = sFileName.replace('.csv', '')\n",
    "    sConvertedDataSubFolder = f'{CONVERTED_DATA_FOLDER}\\\\{sChannelId}'\n",
    "    os.makedirs(sConvertedDataSubFolder)\n",
    "    \n",
    "\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\timestamps.npy', aTimestamps)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\static_covariates.npy', aStaticCovariates)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\knowns.npy', aKnowns)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\observeds.npy', aObserveds)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\distribution.npy', aDistribution)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\dynamic_digits.npy', aDynamicDigits)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\trend.npy', aTrend)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\seasonality.npy', aSeasonality)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\quantiles.npy', aQuantiles)\n",
    "    np.save(f'{sConvertedDataSubFolder}\\\\deltas.npy', aDeltas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
